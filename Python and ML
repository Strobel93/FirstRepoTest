#######################################################################
# Machine Learning
#######################################################################
# Definition: 
    # mathematic/algorithmic models to find structures and patterns in data
    --> Supervised ML: for training the input is known and the output already known 
        --> classification: categorisierung data in SPECIFIC GIVEN categorys
        --> regression    : predict values of dependent column Y given independent column X based on correlation of X,Y (example in form of a straight)
    --> Unsupervis ML: for the training the input is known, output is unknown
        --> clustering    : grouping data together based in similarities (without given target categories)
        --> Association   : find correlation between different features
        --> Dim. reduct   : reducing the number of columns/features/dimensions without loosing integrity
# General:    
    # Feature engineering: turning string data into numerical data format for analysis purpose  
      
# Model Choice:
      # Model Performance bad: more/less complicated Model, get more training data, more features(columns)
      # Overfitting (Model is biased): too adapted to training data ( deals well with training data, terrible with different datasets)
      # Underfitting (Model has high result variance): too simple solution for too complex data (lack of accucary)        
####################################
# Sci-Kit
#################################### 
##########################  
# Tests and Validation
##########################
  # split data into train and test data   
      X_train, X_test, y_train, y_test = train_test_split(a, b ,test_size=0.25, random_state=0)
  
  # Validate the Model (acc score for, in good case different permutations of train test split)
      # Crossvalidation (do multiplate scores with): change train and test split data to the other one, change splitsize, shuffle data (first 20%, 21-40%,..)
      # depending on goal: take solution with best result, best cost-effectiveness 
        compare_model_score_with_prediction =  accuracy_score(labed_data_from_train_test_split, predicted_data) (1 = good, 0 baaad)
        cross_val_func = cross_val_score(Model, unsplit_x_data, unsplit_y_data)
        train_score, val_score = validation_curve(Modelname_from_Import(), unsplit_x_data, unsplit_y_data)
        training_size, train_lc, val_lc = learning_curve(Modelname_from_Import(), unsplit_x_data, unsplit_y_data)
        grid_model = GridSearchCV(Modelname_from_Import(), predifined_dict_with_paramters) --> grid_model.fit(x,y)
        
  # Feature engineering:
        # each row n new columns, each column represents the value, 1 = true, 0 false, all rows with same value have 1 in the same col
          column_names_for_name_zero_one_array_OR_DF_contruction = vectorizer.get_feature_names()
        ################################################################################
        # Best practice use: pipeline
        # Pipeline: merge different features of the folling into the model alreay
          model = make_pipeline(Imputer(strategy='mean'),PolynomialFeatures(degree=3),Linearregression())
          model.fit(x,y)     model.predict(x2)
        ################################################################################
        # for few text columns, with low distinct: converts to N columns for N distinct values, only a single 1 for each row
          dict_vectorizer = DictVectorizer(sparse=False, dtype=int)
          dict_vectorizer.fit_transform(string_data)
          
        # TF-IDF vectorizer (Term Frequency Inverse Document Frequency): transform text into numerical, used for analysis of string only data
          tf_vectorizer = TfidfVectorizer()
          tf_vectorizer.fit_transform(string_data)
        
        # count word occurences in strings, create column for each word, 1 for each word in string
          vectorizer = CountVectorizer()
          x = vectorizer.fit_transform(string_list)
          data_in_tabular_form = x.toarray()
          
        # derive additional columns from existing ones for comparison, analysis,... (number different exponentials of values, ^1,^2,^3)
          poly = PolynomialFeatures(degree=3)
          poly.fit_transform(values_from_column)
          
        # NaN values to mean,min,max,..:
          impute = Imputer(strategy='mean')
          impute.fit_transform(data_array)
          
##########################        
# ML Models:
##########################
# General Sci-Kit Model Usage
    # X/Y: Predict Y given/based on X 
    model = modeltype()
    model.fit(x_train, y_train)
    predict_y = mode.predict(x_test)
    
# Supervised
    # Classification:
        # Bayes: based on Bayes Theorem (conditional probability), naive = takes input of 2 measures, ignoring all the other columns of the data
            GaussianNB()       --> When you have continuous (decimal) features
            CategoricalNB()    --> textdata columns (categories) transformed into numerical ones, so that values repr categories
            MultinomialNB()    --> describes propability of observing counts for categories (often applied to text data)
            BernoulliNB()      --> binary/boolean features (True/False, 0/1)
            
        # Regression: predict value Y based on correlation between X and Y (x can be one dimensional [] or n Dimensional [[],[]])
            LinearRegression() --> assumes linear relationship between Y and Y, tunes the correlation factor to minimize the error (over/underfitting)
            Ridge()            --> Linear + attempt to counter over/underfit by penalizing outliers (high coeffizinets(99x vs 2y)) 
            Lasso()            --> Linear + attempt to counter over/underfit like Ridge() but penalizing up to removing them completly (ridge keeps all)
        
            
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    

  
