#######################################################################
# Big O Notation
#######################################################################
# Measure of Algorithm efficiency analog to the number of data (time complexity)
# O(Factor) = growth/factor/increase of time complexity compared to number of data
# First Order, Order N,...
    O(1): runtime completly independent of number of data 
    O(N): linear runtime, time complexity increases at same rate as number of data (single loop)
    O(N^2): exponential runtime, time complexity rises squared of amount of data (nested foor loop for example)
    ...


#######################################################################
# Machine Learning
#######################################################################
# Usage: https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
# Definition: 
    # mathematic/algorithmic models to find structures and patterns in data
    --> Supervised ML: for training the input is known and the output already known 
        --> classification: categorisierung data in SPECIFIC GIVEN categorys
        --> regression    : predict values of dependent column Y given independent column X based on correlation of X,Y (example in form of a straight)
                --> Simple Linear Regression: straight line prediciton, simple dependecy of x,y (y = ax+b)
                --> Multiple Linear Regression: straight line, based on multiple dependencies x1,x2,xn,y (y = ax1 + ax2 + b)
                --> Polynomial Regression: curve due to non linear relation between x and y variable, by exponenting  (y = ax1^1 + ax2^2 +b)(overfitting!!!)
                --> Logistic Regression: binary classification, weighting values resulting between 0-1, classification dependent on < or > than 0.5
                                         is considdered linear, but instead of straight line it assumes growth in s form (slow begin, fast middle, slow end) 
                --> Generalized Regression: allows assumption of expentiell instead of linear correlation between x,y (linear = normal distribution) 
        --> SUPPORT VECTOR : maximising distance between seperator and data (categories/groups)
            --> SV Classifiers: define a seperator (line,plane) to find the least missclassifications (outliers)
            --> SV Regression:  build more flexibel Model, learning is not based on clear output, but more flexible ranges within these margins values are correct
        --> SUPPORT VECTOR MACHINES: transform data into higher dimension for better seperation (1D --> 2D, x--> x,x^2) and run support vector on it
    --> Unsupervis ML: for the training the input is known, output is unknown
         --> Dangers Unsupervised: local minima/maxima/wrong number of clusters due to random initiasation/chosen number of clusters
            --> clustering    : grouping data together based in similarities (without predefined categories)
            --> Association   : find correlation between different features
            --> Dim. reduct   : reducing the number of columns/features/dimensions without loosing integrity
    --> Reinforcement Learning: true/false feedback for trial/error attempts by the model (input known, no output)

# Data Cleaning and Normailsation:
    # Outliers: missleading values influencing averages
    # Missing: replace, delete
    # Irrelevant Data: dataset including irrelevant categories (regions, groups, ..)
    # Inconsistency: synonyms, typos, different ways to type/format
    # Data Structure: replacing text values with numeric, pivot/unpivot tables

# General:    
    # Cross Validation (CV): RidgeCV, GridSearchCV, XXXCV,...
    # Feature engineering: turning string data into numerical data format for analysis purpose
    # Outliars: direct models in wrong direction, because if they work with distances (stand deviation) outliars missdirect the averages
    # Gradient Descent
        # Machine Learning Optimization technique for model training to deal with huge amount of data, loss gradient --> depends on chosen loss function
        # Implemented by using the slope/gradient (of the loss) to determine the stepsize/learningrate between each data point (big steps far away, small when close)
        # Gradient Descent vs Stochastik Gradient Descent: GD updates model after evaluating all datasets, Updates Model after each row (faster)
    # Learning Rate = Tuning Parameter for Algorithm optimization that determines step size of iteration 
    # Regularization = testing if variables are relevant by adjusting their coefficients (to 0 depending on l1,l2 regul) to check their impact (example lasso,ridge) 
    # Kernel: methof of transforming data into higher dimension to make them useable (for non-linear problems) (1D-->2D, 2D --> 3D)
        # Linear: multiplying x*y + some constant c to create new attribute/feature/column
        # Polinomial Kernel: when only X data, make x^2 the Y data, x^3 the z data, one dimension becomes 3 dimension and more option for seperation (line,plane)
        # Radio Basis Function Kernel RBF: KNN Classification for 
    # Bias/Variance Trade Off: Model Choice depending on problem  
      # BIAS: inability to capture REAL relationship between attributes (example linear regression, straight line not accurate)
      # Variance: difference of accucary between different data sets (train, test) --> high variance = accurate train, inaccurate test prediction (overfit)
      
# Model Choice:
      # Model Performance bad: more/less complicated Model, get more training data, more features(columns)
      # Overfitting (Model is biased): too adapted to training data ( deals well with training data, terrible with different datasets)
      # Underfitting (Model has high result variance): too simple solution for too complex data (lack of accucary)        
      
      
####################################
# Sci-Kit
#################################### 
# Estimator = Model
# Transformer/Preprocessing = Prepare Dataset into fitting form for Model  
##########################  
# Tests and Validation
##########################
  # split data into train and test data   
      X_train, X_test, y_train, y_test = train_test_split(a, b ,test_size=0.25, random_state=0)
  
  # Validate the Model (acc score for, in good case different permutations of train test split)
      # Crossvalidation (do multiplate scores with): change train and test split data to the other one, change splitsize, shuffle data (first 20%, 21-40%,..)
      # depending on goal: take solution with best result, best cost-effectiveness 
        compare_model_score_with_prediction =  accuracy_score(labed_data_from_train_test_split, predicted_data) (1 = good, 0 baaad)
        cross_val_func = cross_val_score(Model, unsplit_x_data, unsplit_y_data)
        train_score, val_score = validation_curve(Modelname_from_Import(), unsplit_x_data, unsplit_y_data)
        training_size, train_lc, val_lc = learning_curve(Modelname_from_Import(), unsplit_x_data, unsplit_y_data)
        grid_model = GridSearchCV(Pipeline/Modelname_from_Import(), predifined_dict_with_paramters) --> grid_model.fit(x,y)
        
  # Feature engineering:
    # Converting String to Vectors, using the new Vectors for transformation
        # each row n new columns, each column represents the value, 1 = true, 0 false, all rows with same value have 1 in the same col
        # new sparse matrix created from this can be used as x for model.fit(x,y)
          column_names_for_name_zero_one_array_OR_DF_contruction = vectorizer.get_feature_names()
        ################################################################################
        # Best practice use: pipeline
        # Pipeline: do all in one step: list of datatransformations on data + one model
          model_pipeline = make_pipeline(StandardScaler(), VarianceThreshold(), KNeighborsClassifier())
          model_pipeline = Pipeline([
                                                ('scaler', StandardScaler()),
                                                ('selector', VarianceThreshold()),
                                                ('classifier', KNeighborsClassifier())
                                    ])
          model_pipeline.fit(x,y)     
          model_pipeline.predict(x2)
        ################################################################################
        # Vectorizers
        #######################
        # for few text columns, with low distinct: converts to N columns for N distinct values, only a single 1 for each row
          dict_vectorizer = DictVectorizer(sparse=False, dtype=int)
          dict_vectorizer.fit_transform(string_data)
          
        # TF-IDF vectorizer (Term Frequency Inverse Document Frequency): transform text into numerical, used for analysis of string only data
          tf_vectorizer = TfidfVectorizer()
          tf_vectorizer.fit_transform(string_data)
        
        # count word occurences in strings, create column for each word, 1 for each word in string (dictionary with structure: 'word':count,..)
          vectorizer = CountVectorizer()
          x = vectorizer.fit_transform(string_list)
          columns_to_tabular_data = vectorizer.vocabulary_
          data_in_tabular_form = x.toarray()
          
        # derive additional columns from existing ones for comparison, analysis,... (number different exponentials of values, ^1,^2,^3)
          poly = PolynomialFeatures(degree=3)
          poly.fit_transform(values_from_column)
          
        # NaN values to mean,min,max,..:
          impute = Imputer(strategy='mean')
          impute.fit_transform(data_array)
          
        # Using get Dummies from Pandas:
        one_dim = ['a', 'b', 'c', 'a']
        multi_dim = pd.DataFrame({'A': ['a', 'b', 'a'],
                                  'B': ['b', 'a', 'c']})
        one_dim_pivot = pd.get_dummies(one_dim)
        all_columns = pd.get_dummies(multi_dim)
        only_columns = pd.get_dummies(multi_dim['A'])
        
        #######################
        # Transformers
        #######################
        transformer = TfidfTransformer()
        x_is_vectorized_data = transformer.fit_transform(x).toarray()
        
        
        
##########################
# Sample Data
##########################
# Make Blobs
X, y_true = make_blobs( n_samples=300     # Number of Rows
                      , centers=4         # Number of Centers
                      , n_features= 3     # Number of colums per row (default 2)
                      , cluster_std=0.60  # standard deviation
                      , random_state=0)   # number = reproduceable values

# Seaborn Datasets
sns.get_dataset_names()
iris = sns.load_dataset("iris")
# scikit
from sklearn.datasets import  load_iris

##########################        
# ML Models:
##########################
# General Sci-Kit Model Usage
    # X/Y: Predict Y given/based on X 
    model = modeltype()
    model.fit(x_train, y_train)
    predict_y = mode.predict(x_test)
    GridSearchCV(model, parameters).fit(x,y)
    make_pipeline(StandardScaler(), VarianceThreshold(), KNeighborsClassifier())
    scores = cross_val_score(model, x, y, cv=k_fold)
    scores = cross_val_score(pipeline, x, y, cv=k_fold)
    model.fit_transform(x) --> combines model.fit(x), mode.transform(x)

#Transformation:
    # GridSearchCV(model, parameters) --> execute model with different list of parameters
    # StandardScaler().fit(data), scaler.transform(data) --> standardizes values (range) for all values with forumula: (data-average)/standard deviation
    

# Supervised
    # Classification:
        # Bayes: based on Bayes Theorem (conditional probability), naive = takes input of 2 measures, ignoring all the other columns of the data
            GaussianNB()       --> When you have continuous (decimal) features
            CategoricalNB()    --> textdata columns (categories) transformed into numerical ones, so that values repr categories
            MultinomialNB()    --> describes propability of observing counts for categories (often applied to text data)
            BernoulliNB()      --> binary/boolean features (True/False, 0/1)
            SVM.SVC()          --> Support Vector Regression/Classification (kernel = 'definition' ['linear','polynomial','rbf','sigmoid '])
            SGDClassifier      --> allows execution of differnet models (loss = '', default linearreg) in combination with Gradient Descent --> fast for much data
            Perceptron()       --> SGDClassifier, updating only on mistakes --> faster 
        # Regression: predict value Y based on correlation between X and Y (x can be one dimensional [] or n Dimensional [[],[]])
            LinearRegression() --> assumes linear relationship between Y and Y, tunes the correlation factor to minimize the error (over/underfitting)
            Ridge()            --> Linear + attempt to counter over/underfit by penalizing outliers (high coeffizinets(99x vs 2y)) --> all variables more linear/even
            Lasso()            --> Linear + attempt to counter over/underfit like Ridge() but penalizing up to removing them completly (ridge keeps all)
            ElasticNet()       --> Ridge + Lasso
            Lars()             --> Least Angle Regression --> find most correlating attribut to target (avg if multiple)--> coninuis linear graph in this direction
            SVM.SVR()          --> Support Vector Regression/Classification (kernel = 'definition' ['linear','polynomial','rbf','sigmoid '])
            SGDRegressor()     --> very large datasets because of efficiency due to gradient descent learning factor
# Unsupervised
    #Clustering:
        # K-Means: creates K-Clusters, guess k-Random-Clusters, assign all to closest  center, create new average center, reassign, new center,....
        # GMM (Gausian Mixture Model): (better)assigns center based on gausian (dependent) probability, chose center, new probility, new center, new prob...
            KMeans(n_clusters=K) 
            GMM(n_components=K)
        

         
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
    

  
